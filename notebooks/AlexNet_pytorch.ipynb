{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa4a3c2-6e31-44b2-9c7b-90d12715980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../InsulatorsDataSet/03 Extraction/ images...\n",
      "Loading ../InsulatorsDataSet/04 Extraction/ images...\n",
      "Loading ../InsulatorsDataSet/05 Extraction/ images...\n",
      "Loading ../InsulatorsDataSet/06 Extraction/ images...\n",
      "Loading ../InsulatorsDataSet/07 Extraction/ images...\n",
      "Loading ../InsulatorsDataSet/08 Extraction/ images...\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_data\n",
    "import albumentations as A\n",
    "\n",
    "img_data, label_data = load_data(reshape = False, scale_percent = 10, resize_only = True)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit = 360, border_mode = 1, p = 1)\n",
    "])\n",
    "\n",
    "def augmentation(images, labels, transform, spr):\n",
    "  # spr: samples per image\n",
    "  new_images = []\n",
    "  new_labels = []\n",
    "  for i in range(len(images)):\n",
    "    # se agrega la imagen original\n",
    "    new_images.append(images[i])\n",
    "    # se agregan \"spr\" rotaciones de la imagen original\n",
    "    for _ in range(spr):\n",
    "      transformed = transform(image = images[i])\n",
    "      transformed_img = transformed[\"image\"]\n",
    "      new_images.append(transformed_img)\n",
    "    # Se agregan los mismos spr + 1 labels (las imagenes rotadas y la original!)\n",
    "    new_labels += [labels[i]] * (spr + 1)\n",
    "  return new_images, new_labels\n",
    "\n",
    "samples_per_image = 10\n",
    "new_images, new_labels = augmentation(img_data, label_data, transform, samples_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c23ba2-ec46-41e1-b50f-c9f31a5ad6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(813)\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis = axis)\n",
    "    return np.take_along_axis(a, idx, axis = axis)\n",
    "\n",
    "def train_val_test_split(images, labels, N_VALIDATION, N_TRAIN, N_TEST):\n",
    "    \n",
    "    idx = shuffle_along_axis(np.arange(N_VALIDATION + N_TRAIN + N_TEST), 0)\n",
    "    \n",
    "    X_train = images[idx[:N_TRAIN]]\n",
    "    y_train = labels[idx[:N_TRAIN]]\n",
    "\n",
    "    X_val = images[idx[N_TRAIN:N_TRAIN+N_VALIDATION]]\n",
    "    y_val = labels[idx[N_TRAIN:N_TRAIN+N_VALIDATION]]\n",
    "\n",
    "    X_test = images[idx[N_TRAIN+N_VALIDATION:]]\n",
    "    y_test = labels[idx[N_TRAIN+N_VALIDATION:]]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6838fe5-526d-4d95-bf6f-bd9bdcdca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(new_images) / 255.0 # normalize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5249e582-f720-44bd-824a-22641b71453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION = 132\n",
    "N_TRAIN = 1056\n",
    "N_TEST = 132\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(images, np.array(new_labels), N_VALIDATION, N_TRAIN, N_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35ca3ab7-5d1e-42e2-9a80-288efbde2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as transform\n",
    "import tensorflow as tf\n",
    "\n",
    "imageslist = []\n",
    "for img in X_train[:64]:\n",
    "    imageslist.append(transform.to_pil_image(transform.to_tensor(tf.image.resize(img, (227, 227)).numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44d3ccc5-0b74-41d4-9211-e05026064279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d265446-cfae-4915-9e05-c671e11d7801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "1 0 200\n",
      "2 0 400\n",
      "3 0 600\n",
      "4 0 800\n",
      "5 0 1000\n",
      "6 0 1200\n",
      "7 0 1400\n",
      "8 200 0\n",
      "9 200 200\n",
      "10 200 400\n",
      "11 200 600\n",
      "12 200 800\n",
      "13 200 1000\n",
      "14 200 1200\n",
      "15 200 1400\n",
      "16 400 0\n",
      "17 400 200\n",
      "18 400 400\n",
      "19 400 600\n",
      "20 400 800\n",
      "21 400 1000\n",
      "22 400 1200\n",
      "23 400 1400\n",
      "24 600 0\n",
      "25 600 200\n",
      "26 600 400\n",
      "27 600 600\n",
      "28 600 800\n",
      "29 600 1000\n",
      "30 600 1200\n",
      "31 600 1400\n",
      "32 800 0\n",
      "33 800 200\n",
      "34 800 400\n",
      "35 800 600\n",
      "36 800 800\n",
      "37 800 1000\n",
      "38 800 1200\n",
      "39 800 1400\n",
      "40 1000 0\n",
      "41 1000 200\n",
      "42 1000 400\n",
      "43 1000 600\n",
      "44 1000 800\n",
      "45 1000 1000\n",
      "46 1000 1200\n",
      "47 1000 1400\n",
      "48 1200 0\n",
      "49 1200 200\n",
      "50 1200 400\n",
      "51 1200 600\n",
      "52 1200 800\n",
      "53 1200 1000\n",
      "54 1200 1200\n",
      "55 1200 1400\n",
      "56 1400 0\n",
      "57 1400 200\n",
      "58 1400 400\n",
      "59 1400 600\n",
      "60 1400 800\n",
      "61 1400 1000\n",
      "62 1400 1200\n",
      "63 1400 1400\n"
     ]
    }
   ],
   "source": [
    "def create_collage(width, height, listofimages):\n",
    "    cols = 8\n",
    "    rows = 8\n",
    "    thumbnail_width = width//cols\n",
    "    thumbnail_height = height//rows\n",
    "    size = thumbnail_width, thumbnail_height\n",
    "    new_im = Image.new('RGB', (width, height))\n",
    "    ims = listofimages\n",
    "    i = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for col in range(cols):\n",
    "        for row in range(rows):\n",
    "            print(i, x, y)\n",
    "            new_im.paste(ims[i], (x, y))\n",
    "            i += 1\n",
    "            y += thumbnail_height\n",
    "        x += thumbnail_width\n",
    "        y = 0\n",
    "\n",
    "    new_im.save(\"Collage.jpg\")\n",
    "\n",
    "create_collage(1600, 1600, imageslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "925ea13b-3eed-4cac-9fda-00e641a48ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transform\n",
    "import torch.nn as nn\n",
    "\n",
    "preparation = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58ca99b9-8bb2-4ef2-ba6f-b9b1da2e427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnew_images = []\n",
    "for n in new_images:\n",
    "    temp = transform.to_tensor(n)\n",
    "    nnew_images.append(preparation(transform.to_pil_image(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6008f609-ef04-4ec8-95b1-49a5f650122d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\danie/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0dadf0c-a0f5-461d-b92c-ab01aeadeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model.classifier[4] = nn.Linear(4096, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d7cbf9a-bd70-4762-a5f3-4aa4b12530ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model.classifier[6] = nn.Linear(1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f0b9044-20ab-4421-bfed-af61513d52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "#Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizer(SGD)\n",
    "optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46a0697-ff60-43ec-8d3c-d963520563e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating CUDA device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Verifying CUDA\n",
    "print(device)\n",
    "\n",
    "#Move the input and AlexNet_model to GPU for speed if available\n",
    "AlexNet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "127e64bf-ec37-488c-b787-03f9377527b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnew_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f6cf3a0-0d06-42eb-9548-cc3350f51e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5aad281-d3df-4cdd-ad46-714f698621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnew_labels = torch.from_numpy(np.array(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "400407c1-2195-4b49-9890-aabfd600cf34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x36 and 9216x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mAlexNet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\alexnet.py:51\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x36 and 9216x4096)"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for img, label in zip(nnew_images, nnew_labels):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = img.to(device), label.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = AlexNet_model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training of AlexNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
